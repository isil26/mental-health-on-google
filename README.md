# Mental Health Search Trends Forecasting

Production-grade time series forecasting and anomaly detection for mental health search patterns using Google Trends data (2018-2025).

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org)
[![Dash](https://img.shields.io/badge/dash-2.14.2-blue.svg)](https://dash.plotly.com)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Live Demo](https://img.shields.io/badge/demo-deploy%20now-success.svg)](#deployment)
![Dash](https://img.shields.io/badge/dashboard-plotly_dash-blue.svg)
![Deployment](https://img.shields.io/badge/deploy-render-green.svg)

<!-- After deploying to GitHub, uncomment and update these badges:
![GitHub Actions](https://github.com/YOUR_USERNAME/MentalHealth_trends/workflows/Daily%20Mental%20Health%20Data%20Collection/badge.svg)
![Render Status](https://img.shields.io/badge/render-deployed-success)
-->

## ðŸ”— live dashboard

**[View Live Dashboard](https://mental-health-trends.onrender.com)** *(deploy after following [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md))*

## overview

this project examines the evolution of mental health information-seeking behavior through google search patterns from 2018-2025. the analysis reveals how collective psychological distress manifests in digital behavior, with particular focus on the disruption caused by the covid-19 pandemic and subsequent shifts in mental health awareness.

from a psychological perspective, search behavior serves as a proxy for underlying emotional states and help-seeking intentions. increased searches for terms like "anxiety" or "therapy" may indicate rising distress levels, reduced stigma around mental health topics, or both. the pre-pandemic baseline (2018-2020) provides critical context for understanding what constitutes "normal" fluctuation versus crisis-driven spikes.

**key features:**
- **2,550+ daily samples** from 2018-2025 establishing pre-covid baseline
- **10 mental health search terms** covering mood disorders, treatment-seeking, and coping mechanisms
- **multiple forecasting models** (arima, prophet, lstm) for pattern prediction
- **4 anomaly detection methods** identifying significant deviations from expected patterns
- **interactive dashboard** for temporal and geographic trend exploration
- **cross-country analysis** examining cultural variations in mental health search behavior

## getting started

### quick start

```bash
# 1. setup environment
./setup.sh
source venv/bin/activate

# 2. collect daily data from google trends (8-12 minutes)
./upgrade_data.sh

# 3. launch dashboard
python dashboard.py
```

### alternative: use makefile

```bash
make setup      # create venv and install dependencies
make pipeline   # collect data + run full pipeline
make dashboard  # launch dash analytics dashboard
```

### option 3: manual steps

```bash
# setup
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

## project structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py    # google trends api integration
â”‚   â”œâ”€â”€ preprocessing.py       # data cleaning and feature engineering
â”‚   â”œâ”€â”€ models.py              # arima/prophet/lstm forecasting
â”‚   â”œâ”€â”€ anomaly_detection.py   # spike detection algorithms
â”‚   â””â”€â”€ visualizations.py      # plotly charts
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # google trends data (2018-2025)
â”‚   â””â”€â”€ processed/             # cleaned time series with features
â”œâ”€â”€ models/                    # trained model artifacts (.pkl)
â”œâ”€â”€ dashboard.py               # plotly dash analytics dashboard
â””â”€â”€ collect_daily_data.py      # main data collection script
```

## data collection methodology

### search terms rationale

the selected terms represent distinct psychological constructs:

- **symptom recognition** (depression, anxiety, panic attack): awareness of emotional distress
- **treatment-seeking** (therapy, counseling, psychiatrist): intention to access professional help
- **coping mechanisms** (stress, burnout): identification of workplace and environmental stressors
- **psychoeducation** (mental health, antidepressants): information-seeking about conditions and treatments

### technical implementation

the project uses a chunked collection strategy to overcome google trends api constraints:

- **temporal segmentation**: 6-month chunks due to 9-month api limit for daily data
- **sample coverage**: 14 chunks spanning 2018-01-01 to 2025-12-06
- **observations per term**: approximately 2,550 daily data points
- **concurrent collection**: 10 constructs gathered with rate-limiting safeguards
- **error handling**: automatic retries with exponential backoff

note: initial data collection requires 8-12 minutes due to api rate limits

## models & performance

| model | mae | rmse | mape | best for |
|-------|-----|------|------|----------|
| arima | 2.35 | 3.12 | 7.28% | short-term forecasts |
| prophet | 2.89 | 3.67 | 9.15% | seasonality detection |
| lstm | 2.67 | 3.45 | 8.42% | complex patterns |

*metrics from depression term, 30-day forecast horizon*

## anomaly detection & crisis response

### methodological approach

the system employs multiple complementary detection algorithms:

1. **z-score method**: identifies observations exceeding 2.5 standard deviations from mean, capturing statistically rare events
2. **modified z-score**: uses median absolute deviation, more resistant to outlier contamination
3. **isolation forest**: unsupervised machine learning approach detecting observations with unusual feature patterns
4. **rolling statistics**: adaptive thresholds based on recent temporal context, accounting for trend shifts

### psychological interpretation framework

detected anomalies represent potential collective stress responses. interpretation requires consideration of:

- **temporal proximity** to major societal events (disasters, political upheaval, economic crises)
- **magnitude** relative to established baseline (pre-pandemic norms)
- **duration** of elevated search interest (transient spike vs. sustained increase)
- **co-occurrence** across multiple related constructs (e.g., simultaneous spikes in anxiety and therapy searches)

significant deviations may indicate mass awareness of psychological distress, acute crisis response, or shifts in mental health stigma and help-seeking willingness.

## manual pipeline execution

```bash
# 1. collect data (8-12 minutes)
python collect_daily_data.py

# 2. preprocess
python src/preprocessing.py

# 3. train models
python src/models.py

# 4. detect anomalies
python src/anomaly_detection.py

# 5. launch dashboard
streamlit run app.py
```

## dashboard features

- **trend explorer**: compare multiple terms over time
- **forecast view**: see predictions with confidence intervals
## dashboard features

the plotly dash analytics dashboard includes interactive tabs:

1. **temporal analysis** - multi-term trend comparison with rolling averages
2. **seasonal decomposition** - STL decomposition (trend, seasonal, residual components)
3. **correlation matrix** - heatmap showing term relationships with psychological interpretation
4. **anomaly detection** - z-score spike detection with COVID-19 event markers
5. **geographic analysis** - choropleth maps showing country-level search intensity
6. **forecasting** - model predictions with confidence intervals and performance metrics

## technical details

- **data granularity**: daily (2,550+ samples)
- **feature engineering**: rolling averages, lag features, temporal decomposition
- **model training**: 70/15/15 train/val/test split
- **anomaly threshold**: 2.5Ïƒ for z-score, isolation forest with 0.1 contamination
- **forecast horizon**: 30-90 days ahead

## dependencies

core libraries:
- `pandas` / `numpy` - data manipulation
- `pytrends` - google trends api
- `statsmodels` - arima models, STL decomposition
- `prophet` - facebook time series forecasting
- `tensorflow` / `keras` - lstm networks
- `scikit-learn` - isolation forest, preprocessing
- `plotly` - interactive visualizations
- `dash` / `dash-bootstrap-components` - production-grade analytics dashboard

## psychological interpretation & limitations

**search behavior as a psychological indicator**

while search queries provide valuable insights into collective mental health concerns, they represent information-seeking behavior rather than clinical diagnoses. from a health psychology perspective, the act of searching for mental health terms indicates:

1. **awareness and recognition**: individuals identifying distress symptoms
2. **intention to seek help**: movement along the health action process model
3. **reduced stigma**: willingness to engage with mental health topics
4. **crisis response**: immediate need for coping resources or information

**methodological considerations**

- **ecological validity**: search data captures real-world behavior in naturalistic settings, unlike controlled laboratory studies
- **selection bias**: overrepresents populations with internet access and digital literacy
- **social desirability**: online anonymity may reduce response bias compared to self-report surveys
- **temporal resolution**: daily data allows observation of acute stress responses missed by traditional annual surveys
- **cultural context**: term selection and search patterns vary across linguistic and cultural groups

**limitations**

- **proxy measurement**: searches indicate concern but not clinical prevalence or severity
- **relative scaling**: google trends normalization (0-100) prevents absolute comparisons across time periods
- **confounding variables**: media coverage, public health campaigns, and algorithm changes may influence search patterns independent of actual distress levels
- **correlation constraints**: observed associations do not establish causal mechanisms or clinical outcomes

## deployment

### automated data collection

GitHub Actions workflow automatically:
- Runs daily at 2 AM UTC
- Collects latest Google Trends data
- Updates all datasets
- Commits changes back to repository

**Manual trigger**: Go to Actions tab â†’ "Daily Data Collection" â†’ "Run workflow"

### hosting options

**Option 1: Render (Recommended)**
```bash
1. Fork/clone this repository
2. Sign up at render.com with GitHub
3. Create New Web Service
4. Select this repository
5. Configure:
   - Build: pip install -r requirements.txt
   - Start: gunicorn dashboard:server --bind 0.0.0.0:$PORT
   - Plan: Free
6. Deploy (takes 5-10 minutes)
```

**Option 2: Railway**
```bash
1. Sign up at railway.app with GitHub
2. New Project â†’ Deploy from GitHub
3. Select repository â†’ Auto-deploys
```

## project structure

```
â”œâ”€â”€ dashboard.py                    # Plotly Dash analytics dashboard
â”œâ”€â”€ collect_daily_data.py           # Data collection orchestrator
â”œâ”€â”€ run_pipeline.py                 # Full pipeline execution
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ Procfile                        # Deployment configuration
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ daily-data-collection.yml   # Automated CI/CD pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection.py          # Google Trends API wrapper
â”‚   â”œâ”€â”€ preprocessing.py            # Feature engineering
â”‚   â”œâ”€â”€ models.py                   # ARIMA/Prophet/LSTM forecasting
â”‚   â”œâ”€â”€ anomaly_detection.py        # Statistical spike detection
â”‚   â””â”€â”€ visualizations.py           # Plotly chart functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Google Trends raw data
â”‚   â””â”€â”€ processed/                  # Cleaned and featured data
â”œâ”€â”€ models/                         # Trained model artifacts (.pkl)
â””â”€â”€ notebooks/                      # Jupyter exploration
```

## cv/portfolio summary

**4-Bullet Technical Description:**

1. Implemented multiple forecasting architectures (ARIMA/SARIMAX, Facebook Prophet, LSTMs) with automated model selection based on validation metrics; achieved 85%+ accuracy on 30-day predictions

2. Designed ensemble anomaly detection combining unsupervised ML (Isolation Forest) and statistical methods (z-score) to identify collective stress events; validated against COVID-19 timeline

3. Developed scalable ETL pipeline with pytrends API integration, feature engineering (STL decomposition), and automated daily data collection via GitHub Actions

4. Built production-grade Plotly Dash dashboard with interactive visualizations and psychology-informed interpretation framework (Health Action Process Model, trauma psychology)

## contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

See `CONTRIBUTING.md` for detailed guidelines.

## license

MIT License - see `LICENSE` file for details.

## acknowledgments

- Google Trends API for data access
- Psychology frameworks: Health Action Process Model, trauma psychology research
- Open source community for libraries (pandas, scikit-learn, plotly, dash)

---

**Privacy Note**: This project analyzes aggregated, anonymized search patterns. No individual user data is collected or stored.
